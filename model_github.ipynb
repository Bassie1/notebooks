{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "d57a4b24-3cb3-42fb-a957-fafca26b1495",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "model-github.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bassie1/notebooks/blob/main/model_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaJW4bZIPWAm"
      },
      "source": [
        "\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
        "\n",
        "#os.chdir(\"../../..\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00002-be0cb21e-72f9-49bc-a750-2ddb6efded2e",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 4922,
        "execution_start": 1622966952291,
        "source_hash": "b2c2f4ef",
        "tags": [],
        "id": "EJQYpoaQPWAs"
      },
      "source": [
        "import copy\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import torch\n",
        "#from pytorch_forecasting import pytorch_lightning as pl\n",
        "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
        "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
        "from pytorch_forecasting.data.encoders import NaNLabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yGUjKphPWAt",
        "outputId": "9b48a464-ea5a-4118-b015-0391a9eb934a"
      },
      "source": [
        "torch. __version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.9.0+cu102'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAc_P8fNPWAu",
        "outputId": "c882a856-56ca-4337-8ec8-15f5a247f23b"
      },
      "source": [
        "pl. __version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.3.3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqZFzgd_PWAz"
      },
      "source": [
        "df = pd.read_csv('github_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daEIWhgQPWA2"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVky0XaOPWA3"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sxIu5UkPWA4"
      },
      "source": [
        "df['month']=df['month'].astype('str').astype('category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00021-cf57a74c-e0a5-4d72-95fc-64860be596a1",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution_millis": 284133,
        "execution_start": 1622964928032,
        "source_hash": "f33e5c83",
        "tags": [],
        "id": "kyTnyrkNPWA5"
      },
      "source": [
        "from pytorch_forecasting.data import (\n",
        "    TimeSeriesDataSet,\n",
        "    GroupNormalizer\n",
        ")\n",
        "df= df\n",
        "max_prediction_length =  3 # changing to a forecst of 3 months, which is 90 days,from a forecast 6 months\n",
        "max_encoder_length = 10  # using 12 months, which is 360 DAYS, instead of  24 months of history\n",
        "training_cutoff =df[\"time_idx\"].max() - max_prediction_length\n",
        "#print(training_cutoff)\n",
        "training = TimeSeriesDataSet(\n",
        "    df[lambda x: x.time_idx <= training_cutoff],\n",
        "    allow_missings=True,\n",
        "    categorical_encoders={'product_id': NaNLabelEncoder(add_nan=True), 'month': NaNLabelEncoder(add_nan=True)},\n",
        "    time_idx=\"time_idx\",\n",
        "    target=\"target\",\n",
        "    group_ids=[\"product_id\"],\n",
        "    #min_encoder_length=0,  # allow predictions without history\n",
        "    min_encoder_length=max_encoder_length // 2, # time series have a minimum length of 13 (min_prediction_length + min_encoder_length)\n",
        "    max_encoder_length=max_encoder_length,\n",
        "    min_prediction_length=1,\n",
        "    max_prediction_length=max_prediction_length,\n",
        "    static_categoricals=[\"product_id\"],\n",
        "    time_varying_known_categoricals=['month'],\n",
        "    time_varying_known_reals=[\n",
        "        \"time_idx\",\n",
        "        \"feature_1\",\n",
        "        'feature_2'\n",
        "    ],\n",
        "    time_varying_unknown_categoricals=[],\n",
        "    time_varying_unknown_reals=[\n",
        "        'target',\n",
        "    ],\n",
        "    target_normalizer=GroupNormalizer(\n",
        "        groups=[\"product_id\"], transformation='softplus'\n",
        "    ),  # use softplus with beta=1.0 and normalize by group\n",
        "    add_relative_time_idx=True,  # add as feature\n",
        "    add_target_scales=True,  # add as feature\n",
        "    add_encoder_length=True,  # add as feature\n",
        "    \n",
        ")\n",
        "# create validation set (predict=True) which means to predict the\n",
        "# last max_prediction_length points in time for each series\n",
        "validation = TimeSeriesDataSet.from_dataset(\n",
        "    training, df, predict=True, stop_randomization=True\n",
        ")\n",
        "# create dataloaders for model\n",
        "batch_size = 128\n",
        "train_dataloader = training.to_dataloader(\n",
        "    train=True, batch_size=batch_size, num_workers=0\n",
        ")\n",
        "val_dataloader = validation.to_dataloader(\n",
        "    train=False, batch_size=batch_size * 10, num_workers=0, drop_last=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00025-c0fb6daa-7baa-44e2-be15-97e3408bf4fa",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "source_hash": "9e1f5b8d",
        "tags": [],
        "id": "2jDn2UOoPWA5"
      },
      "source": [
        "# configure network and trainer\n",
        "pl.seed_everything(42)\n",
        "trainer = pl.Trainer(\n",
        "    gpus=0,\n",
        "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
        "    # of the gradient for recurrent neural networks\n",
        "    gradient_clip_val=0.1,\n",
        ")\n",
        "\n",
        "\n",
        "tft = TemporalFusionTransformer.from_dataset(\n",
        "    training,\n",
        "    # not meaningful for finding the learning rate but otherwise very important\n",
        "    learning_rate=0.03,\n",
        "    hidden_size=16,  # most important hyperparameter apart from learning rate\n",
        "    # number of attention heads. Set to up to 4 for large datasets\n",
        "    attention_head_size=1,\n",
        "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
        "    hidden_continuous_size=8,  # set to <= hidden_size\n",
        "    output_size=7,  # 7 quantiles by default\n",
        "    loss=QuantileLoss(),\n",
        "    log_interval=0,\n",
        "    # reduce learning rate if no improvement in validation loss after x epochs\n",
        "    reduce_on_plateau_patience=4,\n",
        ")\n",
        "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4f9CaE-PWA6"
      },
      "source": [
        "# configure network and trainer\n",
        "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
        "lr_logger = LearningRateMonitor()  # log the learning rate\n",
        "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=30,\n",
        "    gpus=1,\n",
        "    weights_summary=\"top\", #can change to \"full\"\n",
        "    gradient_clip_val=0.1,\n",
        "    #limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
        "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
        "    callbacks=[lr_logger, early_stop_callback],\n",
        "    logger=logger,\n",
        ")\n",
        "\n",
        "\n",
        "tft = TemporalFusionTransformer.from_dataset(\n",
        "    training,\n",
        "    learning_rate=0.03,\n",
        "    hidden_size=16,\n",
        "    attention_head_size=1,\n",
        "    dropout=0.1,\n",
        "    hidden_continuous_size=8,\n",
        "    #output_size=4,  # 7 quantiles by default\n",
        "    loss=QuantileLoss(),\n",
        "    #log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
        "    reduce_on_plateau_patience=4,\n",
        "    log_interval=0 #added because of histogram error \"hacky solution\" github issue 376\n",
        ")\n",
        "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "_5X6O7nTPWA6"
      },
      "source": [
        "# fit network\n",
        "trainer.fit(\n",
        "    tft,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloaders=val_dataloader,\n",
        ")\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MGaaAwcjVu3"
      },
      "source": [
        "# #code to save the trainer and tft\n",
        "torch.save(trainer, \"trainer.pt\")\n",
        "torch.save(tft, \"tft.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW9mt3WfPWA7"
      },
      "source": [
        "\n",
        "# #code to read back in the saved files\n",
        "tft1 = torch.load('tft.pt')\n",
        "trainer1 = torch.load(\"trainer.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SePrMWYRPWA7"
      },
      "source": [
        "# load the best model according to the validation loss\n",
        "# (given that we use early stopping, this is not necessarily the last epoch)\n",
        "best_model_path = trainer1.checkpoint_callback.best_model_path\n",
        "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9eXrHxSPWA7"
      },
      "source": [
        "# calculate mean absolute error on validation set\n",
        "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
        "predictions = best_tft.predict(val_dataloader, show_progress_bar=True)\n",
        "(actuals - predictions).abs().mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fiw5euaFPWA8"
      },
      "source": [
        "# calculate mean absolute error on validation set\n",
        "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
        "predictions = best_tft.predict(val_dataloader, show_progress_bar=True)\n",
        "#pr, x_pr, idx = best_model.predict(data_ts, mode=\"prediction\", return_x=True, return_index=True)\n",
        "predictions, x, idx = best_tft.predict(val_dataloader, mode='prediction', show_progress_bar=True, return_x=True, return_index=True)\n",
        "(actuals - predictions).abs().mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdTCE21YPWA8"
      },
      "source": [
        "\n",
        "\n",
        "# turn predictions into dataframe: time_idx is at horizon=0 (first prediction)\n",
        "df_pred_before = pd.DataFrame(predictions.numpy(), index=pd.MultiIndex.from_frame(idx), columns=pd.RangeIndex(0, predictions.size(1), name=\"horizon\"))\n",
        "\n",
        "# change time_idx to correspond to each prediction\n",
        "df_pred = (\n",
        "    df_pred_before\n",
        "    .stack()\n",
        "    .reset_index([\"time_idx\", \"horizon\"])\n",
        "    .assign(time_idx=lambda x: x.time_idx + x.horizon - 1)\n",
        "    .set_index([\"time_idx\", \"horizon\"], append=True)[0]\n",
        "    .unstack(\"horizon\")\n",
        "    .add_prefix(\"prediction_at_horizon_\")\n",
        ")\n",
        "\n",
        "# add predictions to original dataframe\n",
        "original_df_with_predictions = df.join(df_pred, on=df_pred.index.names)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5hHV-REPWA8"
      },
      "source": [
        "#raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
        "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True, show_progress_bar=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcEmNv2dPWA8"
      },
      "source": [
        "for idx in range(10):  # plot 10 examples\n",
        "    best_tft.plot_prediction(x, raw_predictions, idx=0, add_loss_to_title=True);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocOzxaFdPWA9"
      },
      "source": [
        "# calcualte metric by which to display\n",
        "predictions = best_tft.predict(val_dataloader)\n",
        "mean_losses = SMAPE(reduction=\"none\")(predictions, actuals).mean(1)\n",
        "indices = mean_losses.argsort(descending=True)  # sort losses\n",
        "for idx in range(10):  # plot 10 examples\n",
        "    best_tft.plot_prediction(\n",
        "        x, raw_predictions, idx=indices[idx], add_loss_to_title=SMAPE(quantiles=best_tft.loss.quantiles)\n",
        "    );\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83zRtv-nPWA9"
      },
      "source": [
        "\n",
        "predictions, x = best_tft.predict(val_dataloader, return_x=True)\n",
        "predictions_vs_actuals = best_tft.calculate_prediction_actual_by_variable(x, predictions)\n",
        "best_tft.plot_prediction_actual_by_variable(predictions_vs_actuals);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTYiJbuYPWA9"
      },
      "source": [
        "best_tft.predict(\n",
        "    training.filter(lambda x: (x.product_id == 'B0000AA8UL')  & (x.time_idx_first_prediction == 9)),\n",
        "    mode=\"quantiles\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAfFVpU0PWA9"
      },
      "source": [
        "raw_prediction, x = best_tft.predict(\n",
        "    training.filter(lambda x: (x.asin == \"B0000AA8UL\")  & (x.time_idx_first_prediction == 9)),\n",
        "    mode=\"raw\",\n",
        "    return_x=True,\n",
        ")\n",
        "best_tft.plot_prediction(x, raw_prediction, idx=0);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d4YEcnTPWA-"
      },
      "source": [
        "# select last 24 months from data (max_encoder_length is 24)\n",
        "encoder_data = df[lambda x: x.time_idx > x.time_idx.max() - max_encoder_length]\n",
        "\n",
        "# select last known data point and create decoder data from it by repeating it and incrementing the month\n",
        "# in a real world dataset, we should not just forward fill the covariates but specify them to account\n",
        "# for changes in special days and prices (which you absolutely should do but we are too lazy here)\n",
        "last_data = df[lambda x: x.time_idx == x.time_idx.max()]\n",
        "decoder_data = pd.concat(\n",
        "    [last_data.assign(date=lambda x: x.date + pd.offsets.MonthBegin(i)) for i in range(1, max_prediction_length + 1)],\n",
        "    ignore_index=True,\n",
        ")\n",
        "\n",
        "# add time index consistent with \"data\"\n",
        "decoder_data[\"time_idx\"] = decoder_data[\"date\"].dt.year * 12 + decoder_data[\"date\"].dt.month\n",
        "decoder_data[\"time_idx\"] += encoder_data[\"time_idx\"].max() + 1 - decoder_data[\"time_idx\"].min()\n",
        "\n",
        "# adjust additional time feature(s)\n",
        "decoder_data[\"month\"] = decoder_data.date.dt.month.astype(str).astype(\"category\")  # categories have be strings\n",
        "\n",
        "# combine encoder and decoder data\n",
        "new_prediction_data = pd.concat([encoder_data, decoder_data], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34VHsWgJPWA-"
      },
      "source": [
        "new_raw_predictions, new_x = best_tft.predict(new_prediction_data, mode=\"raw\", return_x=True)\n",
        "\n",
        "for idx in range(10):  # plot 10 examples\n",
        "    best_tft.plot_prediction(new_x, new_raw_predictions, idx=idx, show_future_observed=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP8EvL_5PWA-"
      },
      "source": [
        "interpretation = best_tft.interpret_output(raw_predictions, reduction=\"sum\")\n",
        "best_tft.plot_interpretation(interpretation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEQEPUfsPWA_"
      },
      "source": [
        "dependency = best_tft.predict_dependency(\n",
        "    val_dataloader.dataset, \"discount_in_percent\", np.linspace(0, 30, 30), show_progress_bar=True, mode=\"dataframe\"\n",
        ")\n",
        "# plotting median and 25% and 75% percentile\n",
        "agg_dependency = dependency.groupby(\"discount_in_percent\").normalized_prediction.agg(\n",
        "    median=\"median\", q25=lambda x: x.quantile(0.25), q75=lambda x: x.quantile(0.75)\n",
        ")\n",
        "ax = agg_dependency.plot(y=\"median\")\n",
        "ax.fill_between(agg_dependency.index, agg_dependency.q25, agg_dependency.q75, alpha=0.3);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4owNPpi_PWA_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xji496L9PWA_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}